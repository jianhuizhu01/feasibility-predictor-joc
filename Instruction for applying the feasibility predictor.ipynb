{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 可行性预测器的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载可行性预测器模型\n",
    "- 解析装箱实例\n",
    "- 将实例的特征传递给预测器，得到预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤一：加载可行性预测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = T.jit.load('Feasibility_Predictor.pt')\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "predictor  = predictor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=FFNNFeasibilityChecker\n",
      "  (linear1): RecursiveScriptModule(original_name=Linear)\n",
      "  (linear2): RecursiveScriptModule(original_name=Linear)\n",
      "  (linear3): RecursiveScriptModule(original_name=Linear)\n",
      ")\n",
      "linear1.bias torch.Size([128])\n",
      "linear1.weight torch.Size([128, 17])\n",
      "linear2.bias torch.Size([32])\n",
      "linear2.weight torch.Size([32, 128])\n",
      "linear3.bias torch.Size([1])\n",
      "linear3.weight torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "# 打印模型结构\n",
    "print(predictor)\n",
    "\n",
    "# for name, parameters in predictor.named_parameters():\n",
    "#     print(name, ':', parameters.size())\n",
    "state_dict = predictor.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    print(name, param.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤二：解析装箱实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_manual_feature(items, bin_width, bin_height):\n",
    "    \"\"\"\n",
    "    The method is to extract features from a set of items\n",
    "    Parameters:\n",
    "    ----------\n",
    "    items (a list of np.arrays)\n",
    "    bin_width (int): the width of a bin\n",
    "    bin_height (int): the height of a bin\n",
    "    Returns (a list of metrics)\n",
    "    -------\n",
    "    notes:\n",
    "    we extract five types of features:\n",
    "    1) the ratio between width and height, and four statistical metrics (mean, min, max, std) as the features\n",
    "    2) the ratio between width and the bin width, four statistical metrics (mean, min, max, std) as the features\n",
    "    3) the ratio between height and the bin height, four statistical metrics (mean, min, max, std) as the features\n",
    "    4) the ratio between area of a item and the bin capacity, four statistical metrics (mean, min, max, std) as the features\n",
    "    5) the ratio between total area of the items and the bin capacity, a single metric\n",
    "    \"\"\"\n",
    "    MAX_W_H_RATIO = 18 # 定义了一个常量，用于归一化\n",
    "    capacity = bin_width * bin_height\n",
    "    w_h_ratios = np.asarray(list(map(lambda x: x[0] / x[1], items))) / MAX_W_H_RATIO\n",
    "    w_bin_ratios = np.asarray(list(map(lambda x: x[0] / bin_width, items)))\n",
    "    h_bin_ratios = np.asarray(list(map(lambda x: x[1] / bin_height, items)))\n",
    "    area_capacity_ratios = np.asarray(list(map(lambda x: (x[0] * x[1]) / capacity, items)))\n",
    "    total_area = np.asarray(list(map(lambda x: x[0] * x[1], items))).sum()\n",
    "    w_h_features = [w_h_ratios.mean(), w_h_ratios.min(), w_h_ratios.max(), w_h_ratios.std()]\n",
    "    w_bin_features = [w_bin_ratios.mean(), w_bin_ratios.min(), w_bin_ratios.max(), w_bin_ratios.std()]\n",
    "    h_bin_features = [h_bin_ratios.mean(), h_bin_ratios.min(), h_bin_ratios.max(), h_bin_ratios.std()]\n",
    "    area_capacity_features = [area_capacity_ratios.mean(), area_capacity_ratios.min(),\n",
    "                              area_capacity_ratios.max(), area_capacity_ratios.std()]\n",
    "    total_area_capacity_features = [total_area / capacity]\n",
    "    extracted_features = [w_h_features, w_bin_features, h_bin_features, area_capacity_features,\n",
    "                          total_area_capacity_features]\n",
    "    result = []\n",
    "    for x in extracted_features:\n",
    "        result.extend(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo instance A, result: infeasible \n",
    "# width, height\n",
    "# 6,4\n",
    "# 17,1\n",
    "# 8,1\n",
    "# 15,2\n",
    "# 2,6\n",
    "# 13,1\n",
    "# 4,7\n",
    "# 7,4\n",
    "# 6,4\n",
    "# 2,5\n",
    "\n",
    "# Demo instance B, result: feasible:\n",
    "# 2,9\n",
    "# 3,4\n",
    "# 2,6\n",
    "# 13,1\n",
    "# 4,7\n",
    "# 7,4\n",
    "# 6,4\n",
    "# 2,5\n",
    "instance = [\n",
    "           # width, height\n",
    "            [2,9], \n",
    "            [3,4],\n",
    "            [2,6],\n",
    "            [13,1],\n",
    "            [4,7],\n",
    "            [7,4],\n",
    "            [6,4],\n",
    "            [2,5]\n",
    "            ]\n",
    "features = extract_manual_feature(instance, bin_width = 20, bin_height = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤三：将实例的特征传递给预测器，得到预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=FFNNFeasibilityChecker\n",
       "  (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "  (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "  (linear3): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1287, 0.0123, 0.7222, 0.2262, 0.2438, 0.1000, 0.6500, 0.1775, 0.5000,\n",
       "        0.1000, 0.9000, 0.2236, 0.0906, 0.0500, 0.1400, 0.0352, 0.7250])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = T.tensor(features, dtype=T.float).to(device)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feasible\n",
      "tensor([0.0026])\n"
     ]
    }
   ],
   "source": [
    "with T.no_grad():\n",
    "    y = predictor(features)\n",
    "y\n",
    "if y > 0.5:\n",
    "    print(\"infeasible\")\n",
    "else:\n",
    "    print(\"feasible\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 关于如何解析.pkl文件并在测试样本上运行可行性预测器的说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取.pkl文件\n",
    "with open(\"./training_samples/TrainingSamples.pkl\", \"rb\") as fp:\n",
    "    training_dataset = pkl.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '2L_VRPTW-25-C1-1-PC2_Batch4.csv0_0.tr', 'label': 0, 'bin': array([20, 10]), 'items': array([[ 2,  6],\n",
      "       [13,  1],\n",
      "       [ 4,  7],\n",
      "       [ 7,  4],\n",
      "       [ 6,  4],\n",
      "       [ 2,  5]]), 'packing_class': 'PC2'}\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemDataset(Dataset):\n",
    "    def __init__(self, dataset_file_path):\n",
    "        super(ItemDataset, self).__init__()\n",
    "        # open the .pkl file, the dataset \n",
    "        with open(dataset_file_path, \"rb\") as fp:\n",
    "            dataset = pkl.load(fp)\n",
    "        self.dataset = {k: v for k, v in dataset.items() if v['label'] == 1 or v['label'] == 0}\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集的大小\n",
    "        \"\"\"\n",
    "        return len(self.dataset.keys())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        返回第i个样本的特征和标签\n",
    "        \"\"\"\n",
    "        sample_idx = list(self.dataset.keys())[i]\n",
    "        sample = self.dataset[sample_idx]\n",
    "        items, label = sample['items'], sample['label']\n",
    "        max_width, max_height = sample['bin']\n",
    "        manual_features = np.asarray(extract_manual_feature(items, max_width, max_height))\n",
    "        return manual_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ItemDataset(\"./testing_samples/HybridClasses.pkl\")\n",
    "# print(dataset.dataset.keys())\n",
    "# print(list(dataset.dataset.keys())[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为模型准备data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(dataset, samplers):\n",
    "    test_loader = DataLoader(dataset, batch_size=1, sampler=samplers[\"test\"])\n",
    "    return test_loader\n",
    "\n",
    "def create_samplers(dataset, val_size=3000, split=True):\n",
    "    \"\"\"Create samplers to randomly sample from the dataset\n",
    "    \"\"\"\n",
    "    dataset_idxs = list(range(len(dataset)))\n",
    "    return {\"test\": SubsetRandomSampler(dataset_idxs)}\n",
    "\n",
    "def eval(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    在给定的模型上进行评估，并计算测试损失和准确率，以验证模型在测试集上的性能。\n",
    "    :param model: 神经网络模型\n",
    "    :param dataloader: 数据加载器\n",
    "    :param criterion: 损失函数\n",
    "    :param device: 设备\n",
    "    :return: 平均测试损失，平均测试准确率、所有真实标签和所有预测标签\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0 # 测试损失\n",
    "    total_correct = 0 # 正确的样本数\n",
    "    n_sample = 0 # 样本总数\n",
    "    all_y = [] # 所有真实标签\n",
    "    all_pred = [] # 所有预测标签\n",
    "    with T.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, y = batch # 获取输入和标签\n",
    "            x, y = x.float().to(device), y.float().to(device)\n",
    "            y_pred = model(x) # 模型对输入数据 x 进行预测\n",
    "            y_pred = y_pred.reshape(-1) # 将预测结果展平\n",
    "            loss = criterion(y_pred, y)\n",
    "            all_y.append(y)\n",
    "            all_pred.append((y_pred >= 0.5).float()) # 将预测结果转换为0或1\n",
    "            test_loss += (x.shape[0] * loss.item()) # 计算测试损失\n",
    "            total_correct += ((y_pred >= 0.5).float() == y).sum().item()\n",
    "            n_sample += x.shape[0]\n",
    "    return test_loss / n_sample, total_correct / n_sample, all_y, all_pred\n",
    "\n",
    "def inference_mode(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    :param model: 神经网络模型\n",
    "    :param test_loader: 测试数据集\n",
    "    :param criterion: 损失函数\n",
    "    :param device: 设备\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    test_loss, test_acc, all_y, all_pred = eval(model, test_loader, criterion, device)\n",
    "    # 打印测试损失和测试准确率，保留三位小数\n",
    "    print(f\"Test loss {test_loss:.3f} Test acc {test_acc:.3f}\")\n",
    "    # 将Tensor元素转换为Python标量\n",
    "    all_y = [x.item() for x in all_y]\n",
    "    all_pred = [x.item() for x in all_pred]\n",
    "    # 打印混淆矩阵\n",
    "    print(confusion_matrix(all_y, all_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对测试样本进行推断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset_file_path):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = ItemDataset(dataset_file_path)\n",
    "    samplers = create_samplers(dataset, split=False)\n",
    "    test_loader = create_loaders(dataset, samplers)\n",
    "    # feature_train_loader\n",
    "    # Load model\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "    print(\"Loading the model\")\n",
    "    model = T.jit.load('Feasibility_Predictor.pt')\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    # Train or Infer\n",
    "    inference_mode(model,test_loader,criterion,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在混合测试样本上运行测试模型，其中包括所有包装类的实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model\n",
      "Test loss 0.121 Test acc 0.958\n",
      "[[19773  1074]\n",
      " [  662 20185]]\n"
     ]
    }
   ],
   "source": [
    "main(dataset_file_path = \"./testing_samples/HybridClasses.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model\n",
      "Test loss 0.268 Test acc 0.882\n",
      "[[5219 1275]\n",
      " [ 176 5588]]\n"
     ]
    }
   ],
   "source": [
    "main(dataset_file_path = \"./testing_samples/PackingClass2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model\n",
      "Test loss 0.153 Test acc 0.945\n",
      "[[4773  214]\n",
      " [ 337 4650]]\n"
     ]
    }
   ],
   "source": [
    "main(dataset_file_path = \"./testing_samples/PackingClass3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model\n",
      "Test loss 0.235 Test acc 0.907\n",
      "[[207   3]\n",
      " [ 36 174]]\n"
     ]
    }
   ],
   "source": [
    "main(dataset_file_path = \"./testing_samples/PackingClass4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model\n",
      "Test loss 0.009 Test acc 1.000\n",
      "[[62720    14]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "main(dataset_file_path = \"./testing_samples/PackingClass5.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
